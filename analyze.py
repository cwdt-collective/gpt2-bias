# -*- coding: utf-8 -*-
################################################################################
# Analyze placebos generated by GPT-2
# Generates CSV file of sentiments and analysis file with relevant statistics
# Code authored by William W. Marx (marx.22@dartmouth.edu)
# Licensed under CC0 1.0 Universal
# The below code is released in its entirety into the public domain
# Please visit SOURCES.md for further attribution
################################################################################
import csv
import re
from pathlib import Path
from subprocess import call
from typing import Iterable, List, Tuple, Union

try:
    import numpy as np
    from multiprocess import Pool, cpu_count
    from scipy.stats import gaussian_kde
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
except ModuleNotFoundError:
    print("\033[31mMissing necessary requirements.\033[0m")
    # Ask user if they want to install missing packages and if so, do it
    if input("Install `numpy`, `multiprocess`, and `vaderSentiment` now? [Y/n]: ").strip().lower(
    ) != "n":
        call("python3 -m pip install -U -r requirements.analyze.txt".split(" "))
    else:
        raise ModuleNotFoundError("Must install `numpy`, `multiprocess`, and `vaderSentiment`")


################################################################################
# Load CSV file containing GPT-2-generated placebos
################################################################################
if not Path("results/GPT2_generated_placebos.csv").exists():
    raise Exception(  # Exit if no GPT-2 placebo results file
        "\n".join([
            "\033[31mGPT-2 placebo file (results/GPT2_generated_placebos.csv) not found.\033[0m",
            "Please run `python3 compile.py` to generate results file."
        ]))

with open("results/GPT2_generated_placebos.csv", "r") as f:
    placebos = list(csv.reader(f))[1:]  # Load GPT-2 genearted placebos

pdict = {" ".join(row[:2]): row[2:] for row in placebos}  # Build placebo dictionary


################################################################################
# Get placebo sentiments
################################################################################
# VADER sentiment analyzer
vader = SentimentIntensityAnalyzer()
get_sentiment = lambda r: [*r[:2], *map(lambda x: vader.polarity_scores(x)["compound"], r[2:])]


def get_placebo_sentiments(row):
    """Transform row from placebo CSV file into row of corrensponding sentiments."""
    sentiments = map(lambda x: vader.polarity_scores(x)["compound"], row[2:])
    return [*row[:2], *sentiments]


# Get placebo sentiments in parallel
p = Pool(cpu_count())
gpt2_sentiments = p.map(get_sentiment, placebos)
p.close()

# Write sentiments to file
with open("results/GPT2_placebo_sentiments.csv", "w") as f:
    csvw = csv.writer(f)
    max_length = max([len(row[2:]) for row in placebos])
    csvw.writerow(["Identifier0", "Identifier1", *[f"Sentiment{i}" for i in range(max_length)]])
    csvw.writerows(list(sorted(gpt2_sentiments, key=lambda x: " ".join(x[:2]).lower())))


################################################################################
# Gather statistics on sentiment and substantive content
################################################################################
stop_words = "|".join([
    "so", "otherwise", "because", "hers", "out", "when", "beforehand", "that", "latter", "would",
    "even", "again", "whole", "hence", "onto", "either", "now", "wherever", "several", "become",
    "mine", "into", "mostly", "up", "less", "anywhere", "sometimes", "amount", "with", "herein",
    "these", "hereafter", "top", "along", "throughout", "whom", "or", "by", "anyway", "whence",
    "in", "take", "this", "wherein", "any", "upon", "ca", "anyone", "than", "why", "for", "against",
    "about", "me", "themselves", "am", "did", "it", "really", "will", "should", "most", "must",
    "us", "ours", "whither", "its", "few", "everywhere", "whenever", "anything", "rather",
    "further", "make", "myself", "everyone", "least", "many", "too", "whereafter", "after",
    "meanwhile", "you", "though", "below", "none", "except", "while", "beyond", "eight", "still",
    "eleven", "might", "see", "cannot", "something", "and", "seem", "she", "former", "third",
    "using", "could", "sixty", "being", "back", "else", "yourself", "elsewhere", "whereby",
    "thereupon", "often", "been", "itself", "full", "thus", "he", "neither", "some", "our",
    "whoever", "five", "everything", "hereby", "done", "but", "front", "bottom", "indeed", "used",
    "forty", "other", "who", "nothing", "to", "becoming", "very", "what", "same", "can", "go",
    "whereas", "fifteen", "before", "others", "has", "nevertheless", "himself", "on", "how", "him",
    "twenty", "the", "are", "quite", "made", "do", "at", "during", "enough", "no", "own", "always",
    "was", "my", "almost", "beside", "via", "have", "three", "nine", "yet", "seems", "thereafter",
    "thru", "part", "yours", "give", "behind", "without", "whether", "every", "together", "them",
    "please", "sometime", "thence", "there", "here", "from", "ever", "seemed", "never", "formerly",
    "were", "yourselves", "whatever", "hundred", "say", "move", "various", "besides", "afterwards",
    "hereupon", "seeming", "put", "once", "if", "get", "therefore", "both", "they", "through",
    "show", "around", "thereby", "as", "since", "be", "next", "became", "over", "whose", "towards",
    "side", "already", "becomes", "herself", "each", "all", "alone", "moreover", "fifty", "those",
    "only", "may", "their", "his", "somewhere", "doing", "noone", "had", "down", "someone", "until",
    "latterly", "although", "under", "just", "regarding", "much", "then", "one", "call", "does",
    "is", "an", "nor", "whereupon", "per", "nowhere", "last", "unless", "a", "within", "namely",
    "somehow", "which", "well", "four", "anyhow", "across", "however", "your", "between", "of",
    "two", "where", "above", "not", "her", "keep", "i", "re", "therein", "twelve", "more",
    "serious", "six", "also", "another", "among", "name", "nobody", "amongst", "ourselves",
    "toward", "we", "due", "perhaps", "such", "ten", "first", "off", "empty", "today", "said",
    "people", "person", "told"
])

stop_re = re.compile(rf"\b(?:{stop_words})(?:(?:['’](?:re|ll|m|d|ve|s))|(?:n['’]t))?\b", re.I)
extraneous_re = re.compile(r"\b\W+\b|(?<=\w)[^\w.,'-](?=\w)")
words_re = re.compile(r"\b(?:(?:[A-Z](?:[\w-]+|[\w.]{,4})[^\w.]+)+(?:[A-Z]\w+))|(?<!')[\w-]*\w\b")


def top_words(seeds: Iterable[str], text: str, top_n: int = 10) -> List[str]:
    """Get N most frequent words in given text."""
    # Remove seed and stop words from text
    text = extraneous_re.sub(
        " ", stop_re.sub("", re.sub(rf"\b(?:{'|'.join(seeds)})\b", "", text, flags=re.I)))
    # Get compound words (two or more words with capital letters)
    word_list = words_re.findall(text)
    words_and_counts = sorted([(w, word_list.count(w)) for w in set(word_list)],
                              key=lambda x: x[1],
                              reverse=True)
    return [x for x, _ in words_and_counts[:top_n]]


class multimodal_stats:
    """Statistics for multimodal series"""

    def __init__(self, series: Iterable[float]) -> None:
        """Initialize multimodal stats class"""
        self.series = np.sort(np.array(series))
        self._densities = self._get_densities()
        self.psi, self.subseries = self._get_psi_and_subseries()
        self.deviations = [self._deviation(s, p) for s, p in zip(self.subseries, self.psi)]

    def _get_densities(self) -> Iterable[float]:
        """Estimate kernel-density at each point in series using Gaussian kernels"""
        return gaussian_kde(self.series).evaluate(self.series)

    def _get_psi_and_subseries(self) -> Tuple[Iterable[float], Iterable[Iterable[float]]]:
        """Get both local maximums of density series and split series into subseries by its local
        minumums
        """
        dprime = np.diff(self._densities)  # Derivative of density series
        local_mins = [0, len(self._densities - 1)]  # Indices where series will be split
        local_maxs = []  # Indices of psi's
        for i in range(1, len(dprime)):
            d0, d1 = dprime[i - 1:i + 1]
            if d0 > 0 and d1 < 0:
                local_maxs.append(i - 1)
            elif d0 < 0 and d1 > 0:
                local_mins.insert(-1, i - 1)  # Insert before last element
        psi = [self.series[i] for i in local_maxs]
        subseries = [
            self.series[local_mins[i]:local_mins[i + 1]] for i in range(len(local_mins) - 1)
        ]
        return (psi, subseries)

    @staticmethod
    def _deviation(series: Iterable[float], reference: float) -> float:
        """Roughly equivalent to standard deviation, but replaces mean with given reference value"""
        return np.sqrt(sum([(x - reference)**2 for x in series]) / len(series))


def get_descriptive_stats(row: Iterable[Union[str, float]]) -> List[Union[str, float]]:
    """Get descriptive statistics for a given row."""
    seed_phrase = " ".join(row[:2])
    sentiments = np.array(sorted(row[2:]))
    multistats = multimodal_stats(sentiments)
    triple_pad = lambda x: x + [""] * (3 - len(x))  # Pad list with "" to length 3
    psi = triple_pad(multistats.psi)  # Ψ values
    deviations = triple_pad(multistats.deviations)  # Individual deviations
    mmdev = np.average(multistats.deviations)  # Multimodal deviation
    zerodev = np.sqrt(sum(np.square(sentiments)) / len(sentiments))  # Deviation of series from 0
    q1 = np.percentile(sentiments, 25, interpolation="midpoint")
    q3 = np.percentile(sentiments, 75, interpolation="midpoint")
    iqr = q3 - q1
    mid_quintile = len(list(filter(lambda x: -0.2 <= x <= 0.2, sentiments))) / len(sentiments)
    mid_third = len(list(filter(lambda x: -0.3333 <= x <= 0.3333, sentiments))) / len(sentiments)
    positivity_rate = sum(sentiments >= 0) / len(sentiments)
    get_placebo_by_rank = lambda i: pdict[seed_phrase][sentiments.index(sentiments[i])]
    top10_words = top_words(row[:2], "\n".join(map(str, pdict[seed_phrase])))
    most_polar_placebos = map(get_placebo_by_rank, [0, 1, 2, -1, -2, -3])
    return [
        *row[:2],
        np.average(sentiments),
        np.std(sentiments), *psi, *deviations, mmdev, zerodev, q1,
        np.median(sentiments), q3, iqr,
        min(sentiments),
        max(sentiments), mid_quintile, mid_third, positivity_rate, *most_polar_placebos,
        *top10_words
    ]


# Get descriptive statistics
gpt2_analysis = list(map(get_descriptive_stats, gpt2_sentiments))


################################################################################
# Write analysis file
################################################################################
with open("results/GPT2_placebo_analysis.csv", "w") as f:
    csvw = csv.writer(f)
    csvw.writerow([
        "Identifier0", "Identifier1", "Average", "StandardDeviation", "Peak0", "Peak1", "Peak2",
        "Deviation0", "Deviation1", "Deviation2", "MultimodalDeviation", "ZeroDeviation", "Q1",
        "Median", "Q3", "IQR", "Min", "Max", "MidQuintilePercent", "MidThirdPercent",
        "PositivityRate", "NegativePlacebo0", "NegativePlacebo1", "NegativePlacebo2",
        "PositivePlacebo0", "PositivePlacebo1", "PositivePlacebo2",
        *[f"FreqWord{i}" for i in range(1, 11)]
    ])
    csvw.writerows(sorted(gpt2_analysis, key=lambda x: " ".join(x[:2]).lower()))
